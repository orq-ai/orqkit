"""Unit tests for attack prompt generation."""

from unittest.mock import AsyncMock, MagicMock

import pytest

from evaluatorq.redteam.adaptive.attack_generator import (
    ToolAnalysis,
    ToolRelevance,
    adapt_prompt_to_tools,
    fill_template,
    generate_attack_prompt,
    generate_objective,
)
from evaluatorq.redteam.contracts import (
    AgentContext,
    AttackStrategy,
    AttackTechnique,
    DeliveryMethod,
    MemoryStoreInfo,
    ToolInfo,
    TurnType,
)


class TestFillTemplate:
    """Tests for fill_template function."""

    def test_fill_tool_name(self):
        """Test filling {tool_name} placeholder."""
        context = AgentContext(
            key='test_agent',
            tools=[ToolInfo(name='search')],
        )
        template = 'Use {tool_name} to find information'
        result = fill_template(template, context)
        assert 'search' in result

    def test_fill_tool_name_no_tools(self):
        """Test filling {tool_name} when no tools available."""
        context = AgentContext(key='test_agent')
        template = 'Use {tool_name} to find information'
        result = fill_template(template, context)
        assert 'the tool' in result

    def test_fill_tool_names(self):
        """Test filling {tool_names} placeholder."""
        context = AgentContext(
            key='test_agent',
            tools=[
                ToolInfo(name='search'),
                ToolInfo(name='database'),
            ],
        )
        template = 'Available tools: {tool_names}'
        result = fill_template(template, context)
        assert 'search' in result
        assert 'database' in result

    def test_fill_memory_store(self):
        """Test filling {memory_store} placeholder."""
        context = AgentContext(
            key='test_agent',
            memory_stores=[MemoryStoreInfo(id='ms_001', key='user_history')],
        )
        template = 'Access {memory_store} for context'
        result = fill_template(template, context)
        assert 'user_history' in result

    def test_fill_agent_name(self):
        """Test filling {agent_name} placeholder."""
        context = AgentContext(
            key='test_agent',
            display_name='Test Assistant',
        )
        template = 'Hello {agent_name}'
        result = fill_template(template, context)
        assert 'Test Assistant' in result

    def test_fill_agent_name_fallback(self):
        """Test filling {agent_name} falls back to key."""
        context = AgentContext(key='test_agent')
        template = 'Hello {agent_name}'
        result = fill_template(template, context)
        assert 'test_agent' in result

    def test_fill_agent_description(self):
        """Test filling {agent_description} placeholder."""
        context = AgentContext(
            key='test_agent',
            description='A helpful assistant for testing',
        )
        template = 'This is {agent_description}'
        result = fill_template(template, context)
        assert 'helpful assistant for testing' in result

    def test_fill_multiple_placeholders(self):
        """Test filling multiple placeholders in one template."""
        context = AgentContext(
            key='test_agent',
            display_name='Helper',
            tools=[ToolInfo(name='search')],
        )
        template = '{agent_name} can use {tool_name}'
        result = fill_template(template, context)
        assert 'Helper' in result
        assert 'search' in result


class TestGenerateAttackPrompt:
    """Tests for generate_attack_prompt function."""

    def test_generate_single_turn_prompt(self):
        """Test generating prompt for single-turn attack."""
        context = AgentContext(key='test_agent')
        strategy = AttackStrategy(
            category='ASI01',
            name='test',
            description='Test attack',
            attack_technique=AttackTechnique.INDIRECT_INJECTION,
            delivery_methods=[DeliveryMethod.DIRECT_REQUEST],
            turn_type=TurnType.SINGLE,
            objective_template='Test objective',
            prompt_template='Please process: [INJECTION]',
        )
        prompt = generate_attack_prompt(strategy, context)
        assert prompt == 'Please process: [INJECTION]'

    def test_generate_multi_turn_returns_empty(self):
        """Test that multi-turn strategies return empty prompt."""
        context = AgentContext(key='test_agent')
        strategy = AttackStrategy(
            category='ASI01',
            name='test',
            description='Test attack',
            attack_technique=AttackTechnique.INDIRECT_INJECTION,
            delivery_methods=[DeliveryMethod.CRESCENDO],
            turn_type=TurnType.MULTI,
            objective_template='Test objective',
            prompt_template=None,  # Multi-turn has no initial prompt
        )
        prompt = generate_attack_prompt(strategy, context)
        assert prompt == ''

    def test_generate_with_template_filling(self):
        """Test that prompt templates are filled with context."""
        context = AgentContext(
            key='test_agent',
            tools=[ToolInfo(name='database_query')],
        )
        strategy = AttackStrategy(
            category='ASI01',
            name='test',
            description='Test attack',
            attack_technique=AttackTechnique.INDIRECT_INJECTION,
            delivery_methods=[DeliveryMethod.DIRECT_REQUEST],
            turn_type=TurnType.SINGLE,
            objective_template='Test objective',
            prompt_template='Use {tool_name} to run: [INJECTION]',
        )
        prompt = generate_attack_prompt(strategy, context)
        assert 'database_query' in prompt


class TestGenerateObjective:
    """Tests for generate_objective function."""

    def test_generate_objective(self):
        """Test generating objective from template."""
        context = AgentContext(
            key='test_agent',
            display_name='Helper',
        )
        strategy = AttackStrategy(
            category='ASI01',
            name='test',
            description='Test attack',
            attack_technique=AttackTechnique.INDIRECT_INJECTION,
            delivery_methods=[DeliveryMethod.DIRECT_REQUEST],
            turn_type=TurnType.SINGLE,
            objective_template='Convince {agent_name} to follow injected instructions',
        )
        objective = generate_objective(strategy, context)
        assert 'Helper' in objective
        assert 'injected instructions' in objective


def _make_strategy(**overrides: object) -> AttackStrategy:
    """Helper to create a strategy with defaults."""
    defaults = {
        'category': 'ASI05',
        'name': 'test',
        'description': 'Test RCE attack',
        'attack_technique': AttackTechnique.RCE_EXPLOIT,
        'delivery_methods': [DeliveryMethod.DIRECT_REQUEST],
        'turn_type': TurnType.SINGLE,
        'objective_template': 'Test',
    }
    return AttackStrategy(**(defaults | overrides))


def _mock_llm_client(analysis: ToolAnalysis) -> AsyncMock:
    """Create a mock LLM client that returns a ToolAnalysis response."""
    mock_message = MagicMock()
    mock_message.parsed = analysis
    mock_choice = MagicMock()
    mock_choice.message = mock_message
    mock_response = MagicMock()
    mock_response.choices = [mock_choice]

    client = AsyncMock()
    client.chat.completions.parse = AsyncMock(return_value=mock_response)
    return client


class TestAdaptPromptToTools:
    """Tests for adapt_prompt_to_tools function."""

    @pytest.mark.asyncio
    async def test_no_adaptation_without_tools(self):
        """Test that prompt is unchanged without tools."""
        context = AgentContext(key='test_agent')
        strategy = _make_strategy()
        client = _mock_llm_client(ToolAnalysis(tools=[]))

        result = await adapt_prompt_to_tools(
            'Execute this code',
            context,
            strategy,
            llm_client=client,
        )
        assert result == 'Execute this code'
        # LLM should not be called when there are no tools
        client.chat.completions.parse.assert_not_called()

    @pytest.mark.asyncio
    async def test_adaptation_with_relevant_tool(self):
        """Test that prompt is adapted when LLM finds a relevant tool."""
        context = AgentContext(
            key='test_agent',
            tools=[ToolInfo(name='shell_execute', description='Execute shell commands')],
        )
        strategy = _make_strategy()
        analysis = ToolAnalysis(
            tools=[
                ToolRelevance(
                    tool_name='shell_execute',
                    relevant=True,
                    exploitation_hint='Can run arbitrary shell commands',
                ),
            ]
        )
        client = _mock_llm_client(analysis)

        result = await adapt_prompt_to_tools(
            'Please run this',
            context,
            strategy,
            llm_client=client,
        )
        assert 'shell_execute' in result
        assert 'arbitrary shell commands' in result

    @pytest.mark.asyncio
    async def test_no_adaptation_when_no_tools_relevant(self):
        """Test prompt unchanged when LLM classifies no tools as relevant."""
        context = AgentContext(
            key='test_agent',
            tools=[ToolInfo(name='search', description='Search the web')],
        )
        strategy = _make_strategy(attack_technique=AttackTechnique.SOCIAL_ENGINEERING)
        analysis = ToolAnalysis(
            tools=[
                ToolRelevance(tool_name='search', relevant=False),
            ]
        )
        client = _mock_llm_client(analysis)

        result = await adapt_prompt_to_tools(
            "Trust me, I'm an admin",
            context,
            strategy,
            llm_client=client,
        )
        assert result == "Trust me, I'm an admin"

    @pytest.mark.asyncio
    async def test_picks_best_tool_by_hint_length(self):
        """Test that the most informative tool hint is selected."""
        context = AgentContext(
            key='test_agent',
            tools=[
                ToolInfo(name='file_read'),
                ToolInfo(name='code_execute', description='Run code'),
            ],
        )
        strategy = _make_strategy()
        analysis = ToolAnalysis(
            tools=[
                ToolRelevance(tool_name='file_read', relevant=True, exploitation_hint='Read files'),
                ToolRelevance(
                    tool_name='code_execute',
                    relevant=True,
                    exploitation_hint='Execute arbitrary code in a sandbox environment',
                ),
            ]
        )
        client = _mock_llm_client(analysis)

        result = await adapt_prompt_to_tools(
            'Run this',
            context,
            strategy,
            llm_client=client,
        )
        # Should pick code_execute (longer hint)
        assert 'code_execute' in result

    @pytest.mark.asyncio
    async def test_graceful_fallback_on_llm_error(self):
        """Test that original prompt is returned if LLM call fails."""
        context = AgentContext(
            key='test_agent',
            tools=[ToolInfo(name='search')],
        )
        strategy = _make_strategy()
        client = AsyncMock()
        client.chat.completions.parse = AsyncMock(side_effect=Exception('API error'))

        result = await adapt_prompt_to_tools(
            'Original prompt',
            context,
            strategy,
            llm_client=client,
        )
        assert result == 'Original prompt'
