[build-system]
requires = [ "hatchling" ]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = [ "src/evaluatorq" ]
include = [ "src/evaluatorq/py.typed" ]

[tool.hatch.build.targets.sdist]
exclude = [ "examples/", ".venv/", "tests/" ]

[tool.pytest.ini_options]
testpaths = [ "tests" ]
python_files = [ "test_*.py", "*_test.py" ]
norecursedirs = [ ".venv", "dist", "*.egg-info" ]

[tool.basedpyright]
reportIncompatibleVariableOverride = "none"
reportExplicitAny = "none"
reportMissingImports = "warning"
reportAny = "none"
reportUnknownMemberType = "none"
reportUnknownArgumentType = "none"
reportUnknownVariableType = "none"
reportUnknownParameterType = "none"
reportMissingTypeStubs = "none"
reportUnusedCallResult = "none"

[project]
name = "evaluatorq"
version = "1.1.0-rc.4"
description = "An evaluation framework library for Python that provides a flexible way to run parallel evaluations and optionally integrate with the Orq AI platform."
readme = "README.md"
requires-python = ">=3.10"
dependencies = [ "pydantic>=2.0", "httpx>=0.28.1", "rich>=14.2.0" ]

  [project.license]
  text = "MIT"

  [project.optional-dependencies]
  orq = [ "orq-ai-sdk>=3.13.16" ]
  otel = [
    "opentelemetry-api>=1.20.0",
    "opentelemetry-sdk>=1.20.0",
    "opentelemetry-exporter-otlp-proto-http>=1.20.0",
    "opentelemetry-semantic-conventions>=0.41b0"
  ]
  langchain = ["langchain>=1.0.0"]


  [project.urls]
  Homepage = "https://github.com/orq-ai/orqkit"
  Repository = "https://github.com/orq-ai/orqkit/tree/main/packages/evaluatorq-py"
  Documentation = "https://github.com/orq-ai/orqkit/tree/main/packages/evaluatorq-py"

[dependency-groups]
dev = [
  "pytest>=8.0.0",
  "pytest-asyncio>=1.2.0",
  "basedpyright>=1.21.0",
  "ruff>=0.14.3",
  "orq-ai-sdk>=3.13.16",
  "opentelemetry-api>=1.20.0",
  "opentelemetry-sdk>=1.20.0",
  "opentelemetry-exporter-otlp-proto-http>=1.20.0",
  "langchain-openai>=1.1.7",
  "langchain>=1.0.0",
  "python-dotenv>=1.2.1",
]
